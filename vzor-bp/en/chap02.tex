\chapter{Related work} \label{ch2:related}
The goal of this work is to designe an AutoML system for pipeline optimization.
As such, we present existing AutoML systems in this chapter to provide insight
into this recently emerged field. There are several approaches, with every system
focusing on a different aspect of the problem. The examples are divided into
two groups, where the first comprises of methods focusing mainly on simple
models and the second of systems which enable more complex architectures.
Finally we compare the systems, which used a similar approach in
pipeline structure representation, with the system designed by us.

\paragraph{Hyperparameter optimization methods} \label{CASH}
The first attempt to solve the CASH problem (defined in section \ref{CASH})
was the AutoML system \emph{Auto-WEKA}
\citep{DBLP:journals/corr/abs-1208-3719}. It employs a Bayesian optimization
method to find the optimal set of hyperparameters. To solve the model selection
at the same time, an artificial hyperparameter which represents the selected
model is added to the system. Other specific hyperparameters are induced from its
value --- for example the hyperparameters of the model itself or subestimators of
ensembles. The same principle holds also for feature preprocessing methods.

The successor of Auto-WEKA, \emph{Auto-sklearn}, improved existing systems by
adding a metalearning step to the algorithm. The optimization step is the same
as in Auto-WEKA, but the algorithm is \emph{warm-started} via a history of most
successful models on similar datasets. In the end, the resulting models are
combined together to a specific ensemble. This is done to take advantage of all
potentially good methods instead of choosing only one (not necessarily the best)
and discarding the rest \citep{Feurer:2015:ERA:2969442.2969547}.
% AutoNet --- based on autoweka, CASH for ANN, architecture as well (via
% parametrization)

Although these systems work quite well in practice, they support only a small
number of model architectures. The algorithms used in Auto-WEKA are general
enough, but the number of subestimators is limited to 5 and only base-learners can
be used. Auto-sklearn abandoned ensembles altogether in the optimization process
and does not support an arbitrary ensemble in the final step of the algorithm.
The reason behind this limitation is that by increasing the model complexity the
configuration space becomes too large.

A similar system has been developed for the purpose of automatic tuning of neural
networks --- Auto-Net \citep{pmlr-v64-mendoza_towards_2016}.

\paragraph{Architecture search}
The search for an ideal model architecture can be understood as a part of the
CASH problem. This subfield of AutoML has recently gained in popularity with the
demand for automated \emph{neural architecture search} (NAS)
\citep{2018arXiv180805377E}. It also encompasses the task of pipeline design.

NAS is the process of optimization of deep neural network architectures. It may be
seen as a subfield of AutoML, as along with the architecture search it may perform
a hyperparameter optimization. It has already been
successful in some problem classes like image recognition or object detection, where
its architectures outperformed those designed by hand. In the paper by 
\cite{2018arXiv180805377E}, the NAS methods are classified according to three
dimensions --- search space, search strategy and performance estimation strategy.
The search space is understood as all possible neural network architectures, that
can be created with the automatic method. The search strategy classifies the method
used for architecture optimization --- some examples are the already mentioned
evolutionary algorithms and Bayesian optimization, as well as the reinforcement
learning or random search. Finally, the performance estimation strategies are aimed
at reduction of running time. These include `lower fidelities estimates', for example
training on subsets of data or for fewer epochs. Another option is the `learning
curve extrapolation'. Finally, a mate-learning like approach is to warm-start the
new networks by setting the weights according to previous networks, while focusing
on developing new architectures.

There are only few systems that allow unlimited pipeline sizes. The system
\emph{RECIPE} uses grammar-based GP (another subfield of GP which utilizes grammars
to constraint the tree structure) to evolve pipelines, but still limits the
size of the pipeline \citep{10.1007/978-3-319-55696-3_16}.
There are two systems which support arbitrary-sized pipelines. One of them is
\emph{TPOT}, which uses genetic programming to evolve tree-based models. The
root of the tree represents a single estimator whereas the branches are feature
preprocessing methods \citep{Olson2016EvoBio}.
The other system --- \emph{ML-Plan} --- is based on hierarchical planning
\citep{Mohr2018}. Both of the methods support a chain of feature preprocessors,
including feature union and stacking (ensemble methods which construct new
features from subestimators).

Nevertheless, none of these methods support more complex ensemble structures. The
system GP-ML developed by \cite{Kren2017AutomaticCO} uses the strongly typed
functional programming to develop an arbitrary pipeline structure. This type of GP
is partly similar to the developmental GP, as every node represents a function.
However, the constraints on the structure are specified in the language of
functional programming, which is very different from our approach.

\paragraph{Comparison of our system with TPOT}
Our system relates most to TPOT, as it uses genetic programming to optimize
the pipelines. However, instead of using the simple GP like TPOT does, we use
the developmental GP which enables us to create more complex ensemble
structures. For instance, TPOT uses only the stacking ensemble, which combines the
output of its base-estimator with the input data, whereas in our work we use for
example the voting ensemble, which combines the output of several base-estimators.
However, the main difference is that in our system, the pipelines are composed
from a feature preprocessor chain and of an estimator, which may be in fact another
pipeline with the same structure. In TPOT, there is also a feature preprocessor
chain, but at the end of the pipeline must be a simple base-estimator. As such, it
is impossible to create nested structures as in our case.

TPOT also focuses more on the feature preprocessing step of the 
workflow, providing additional built-in transformers beyond the scikit-learn 
provided algorithms, for example a more scalable OneHotEncoder or the stacking
ensemble. TPOT uses either a standard tournament selection or 
NSGA-II and multiobjective fitness. The latter is however composed from the 
score and the tree size, whereas ours uses the score and evaluation time.

The three dimensions of the optimization may be partly applied to pipeline
optimization as well. For example, the search space of our method are all pipelines,
whereas the TPOT operates on the search space of tree based pipelines. In both cases,
the search strategy is the genetic programming, more precisely the developmental GP
in our case. Lastly, both performance estimation strategies of TPOT and of our
system are based on `lower fidelities' --- our model proposes sampling strategies and
reduces the instance count, whereas in TPOT there has been recently introduced a
special biologically inspired feature selection that significantly reduced the
running time \citep{Le502484}.

\paragraph{Comparison of our system with GP-ML}
The idea behind GP-ML is to separate the actual ensemble methods from the splitters
and combinators. A splitter is a method that takes input data and splits or copies them
into new data; the result is then passed to base-estimators. The combiner in turn
combines the output of the estimators into a final output.

Compared with our system, this approach is more general, as in our case both
splitting and combining is implicitly defined by a specific ensemble, which acts as
a black box. Thus, if we want to combine a specific splitting strategy with a
combining function, we must define a new ensemble. On the other hand, as the
developmental GP has a simpler constraint interface, the extensibility is probably
easier. Moreover, it is more easily convertible to scikit-learn pipeline API, which
opens up the optimization space for new methods. Lastly, to represent a pipeline as
an individual of GP-ML, it is necessary to utilize more nodes than in our case.
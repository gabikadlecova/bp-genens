\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

The goal of our work was to develop an AutoML system for automatic
pipeline optimization. Using the developmental GP, we created a general encoding
of the scikit-learn pipelines that converts a DAG pipeline into a tree representation.

The encoding was designed in an extensible way. The list of used methods can be easily 
extended with more ensembles and methods that comply to the scikit-learn API.
We also implemented a tree individual that, compared to the DEAP tree individual, supports
variable arity of nodes.

We introduced several bloat-reducing approaches. The grow method of the GP was
modified to initialize less branched trees, which were more suitable for representation
of pipelines. We also employed a weighted node selection, which prefers simpler methods.
That way, we avoided nested ensembles which are computationally demanding, but do not
significantly improve the score. 

To decrease the running time of the optimization, we implemented some performance
estimation strategies based on sampling. A sample of the input dataset was either
created once per generation or per individual evaluation. The two strategies were
then tested in the first experiment on two medium size dataset and one large dataset.
The results of the experiment did not show any noticeable difference in
the two approaches, but implied that by generating a sample for every evaluation, we
increase the variance of results.

For the evolutionary optimization itself, we designed specific genetic operators. The
subtree crossover and subtree mutation greatly alter the structure of the pipeline.
The point mutation changes only one method of the pipeline while preserving the
architecture. Finally, the node argument mutation changes a random hyperparameter of
a random node, thus targeting the CASH problem.

To test whether the absence of a certain genetic operator influences the final result,
we carried out the second experiment, where we turned the operators off one at a time.
With this settings, we have done several runs on a small dataset and on a medium sized
unbalanced dataset. The results of the experiments have shown that on the medium
sized dataset, the hyperparameter mutation is essential for obtaining good results.
On the second dataset, turning off the point mutation produced significantly better
results. This behaviour is to be subject of future research. Finally, in all cases
the results were better than the simple case where all genetic operators were turned
off.

In the last experiment, we evaluated the performance on the system on the OpenML-CC18 benchmark suite.
We ran the evolutionary optimization on each of the 72 datasets of the
benchmark, while using one of the sampling strategies. Three best pipelines were evaluated
and the maximum of the scores was chosen as the final output. We then compared our
results with the statistic of runs uploaded to OpenML. Overall, on most datasets
our system performed better than the upper quartile of OpenML runs or only slightly
below it. In one case, our system outperformed all runs uploaded to OpenML.

In future extensions of our system, there are several concepts which are to be
explored. As has been shown in the second experiment, the mutation of hyperparameters
may significantly improve the results. As such, it would be suitable to employ a
better optimization strategy that a simple selection of values from a list. One
possibility would be to use the evolutionary strategies, which is a subfield of
evolutionary algorithms particularly suited for continuous optimization. Another
option is to create a Bayesian optimization method inspired by existing AutoML
systems. Moreover, some additional methods may yield promising results, notably
hill-climbing or mutation of multiple hyperparameters at once.
As shown in experiment 2, it is necessary to further optimize the genetic operators.
Again, more sophisticated methods could lead to interesting pipeline architectures.
A related problem is the setting of weights used for initialization, which should be
thoroughly explored.

To extend the methods used in the pipelines, a stacking estimator should be added
to the primitive set, as it may lead to very promising results. Also, we should focus
more on the data preprocessing methods, as users of this system still need to perform
some initial encoding and imputation of missing data. Our system should be then
compared with existing AutoML methods on suitable benchmark data.



% Future work

% focus more on the data preprocessing part
% fut work - time measurements
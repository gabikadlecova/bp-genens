\chapter{Our solution} \label{our:solution}
% TODO insert \nopagebreak after \midrule s

In our solution we design an AutoML system for workflow optimization based on
developmental genetic programming. Compared to existing systems it supports 
arbitrary-sized pipelines as well as complex ensemble structures. An overview
of the process is shown in algorithm \ref{alg:genens}. In section
\ref{genens:devGP} we describe how we apply the developmental GP to this
problem (line \ref{line:devGP}). Evaluation of pipelines (line \ref{line:compile})
and implementation details are presented in section \ref{genens:eval}.

\begin{algorithm}
\DontPrintSemicolon 
\caption{Pipeline optimization --- main\label{alg:genens}}
  \KwData{dataset $d$, configuration $c$}
  \KwResult{optimized pipelines}
  \SetKwFunction{Compile}{Compile}
  \;
  $individuals \longleftarrow$ run developmental GP on $d$ with $c$ \label{line:devGP} \;
  $pipelines \longleftarrow$ \Compile{$individuals$} \label{line:compile}
  \;\;
  \KwRet{pipelines}
  
\end{algorithm}

% POHLED ZVRCHU shrnout vse, detaily dole


% problem
% reseni generuji dle dev GP

% algo

% indiv. preloz do sklearn pipeliny, evaluuj ji na datech


% ROzepsanej GP algo do casti 3.1
% odkaz z textu do algu
% sem nekam dat crossval!!! - procedura 'vyhodnot jedince'

\section{Evolutionary optimization of pipelines} \label{genens:devGP}
In this section, we describe the necessary components of the genetic algorithm.
The process corresponds to the schema of a general genetic algorithm
\ref{alg:EA}. A more detailed schema is presented in algorithm \ref{alg:devgp}.
Individuals of this particular EA are pipelines encoded as trees via
developmental GP; the encoding is summarized in section \ref{sec:encoding}.
The initialization procedure (line \ref{alg:genens:init}) is explained in
section \ref{sec:init}. All reproduction operators
(line \ref{alg:genens:repro}) are presented in section \ref{sec:repro}.
Finally, the selection and fitness are specified in section \ref{sec:fitsel}.

\begin{algorithm}
\DontPrintSemicolon 
\caption{Pipeline optimization --- developmental GP \label{alg:devgp}}
  \KwData{population size $k$, maximum number of generations $max\_gen$,
  crossover probability $p_{cx}$,
  mutation probabilities $p_{mut}$, $p_{mut\_node}$, $p_{mut\_args}$ }
  \KwResult{evolved tree individuals}
  \SetNoFillComment
  \SetKwFunction{Cx}{crossover}
  \SetKwFunction{Mut}{mutation}
  \SetKwFunction{Mutnode}{node\_mutation}
  \SetKwFunction{Mutarg}{arg\_mutation}
  \SetKwFunction{Eval}{evaluate}
  \SetKwFunction{Compile}{compile}
  \;
 
  \Fn{\Eval}{ \label{alg:genens:eval}
        $pipe \longleftarrow$ \Compile($ind$) \;
        $score, time \longleftarrow$ evaluate $pipe$ on a sample\;
        $ind.fitness \longleftarrow (score, \log\mleft(time\mright))$
  }
  \;
  \tcc{run developmental GP}  
  $P(0) \longleftarrow$ initialize population of GP trees \label{alg:genens:init}
  \;\;
  \While{$n < max\_gen$}{
      \;
      \tcc{evaluate individuals}
      \For{$ind$ in $P(n)$} {
         \Eval{$ind$}
      
         
      }
      \;
      \tcc{reproduction} \label{alg:genens:repro}
      \For{$i$ in \Range{$k/2$}} {
         $i_1, i_2 \longleftarrow$ tournament selection from $P(n)$

         \If{$p_{cx}$} {
            \Cx{$i_1$, $i_2$} \tcc*[r]{subtree crossover}
         }
         \;
         \If{$p_{mut}$ for $k=1,2$} {
            \Mut{$i_k$} \tcc*[r]{subtree mutation}
         }
         \If{$p_{mut\_node}$ for $k=1,2$} {
            \Mutnode{$i_k$} \tcc*[r]{node mutation}
         }
         \If{$p_{mut\_args}$ for $k=1,2$} {
            \Mutarg{$i_k$} \tcc*[r]{hyperparameter mutation}
         }
         
         \;
         add $i_1, i_2$ to offspring population $P_o(n)$
      }
      \;
      $P(n+1) \longleftarrow$ NSGA-II selection from $P_o(n)$
  }
  \;
  \KwRet{Pareto front of $P(c)$} 
\end{algorithm}


\subsection{Individual encoding} \label{sec:encoding}
\begin{figure}[ht]\centering
\includegraphics[width=0.7\textwidth]{../img/pipeline-pdfa.pdf}
\caption{Schema of an example pipeline}
\label{pic02:pipeline}
\end{figure}

The individual encoding is one of the most important parts of this system. With
ensembles and complex feature preprocessing methods like stacking or feature
union, most of the pipelines become in fact directed acyclic graphs (figure~\ref{pic02:pipeline}). Therefore, we cannot directly use the simple tree-based
encoding. Instead we use the developmental GP with cellular encoding described
in section~\ref{devGP}.

In our case, the embryo is an empty pipeline. To create a complex pipeline, we
modify it by inserting steps into it.

\begin{figure}[ht]\centering
    \subfloat[Tree encoding]{{\includegraphics[width=0.25\textwidth]{../img/ada.png} }}%
    \qquad
    \subfloat[Encoded pipeline]{{\includegraphics[width=0.5\textwidth]{../img/ada-pdfa.pdf} }}%
    \caption{An example pipeline encoded to a tree individual}%
    \label{pic:pipeencoding}%
\end{figure}

The process can be demonstrated on figure
\ref{pic:pipeencoding}. The root of the tree represents the embryo which will
be modified by subsequent operations. In this case, the left subtree modifies
the ensemble structure whereas the right subtree modifies the feature
preprocessor chain. The pipeline contains only one preprocessor, hence the
right subtree is terminated by the corresponding node. The left son can be
either an ensemble or a simple method. Here it is the AdaBoost ensemble which
has one base classifier. The subestimator is again a pipeline, which is composed
of a MinMaxScaler and Stochastic gradient descent classifier. The specific
hyperparameter of every pipeline step are stored aside the nodes and are not
depicted in the figures.

Table \ref{tab03:nodes} lists the nodes used in the current implementation of
our system. Input type is defined as a cartesian product of types, output type
is a single type; terminals have only the output type defined. Output types of
child nodes must match the input type of parent node. The list is extensible,
it is possible to add a definion of similar nodes, e.g. a different ensemble
flavour like stacking. The nodes that are specific for a given estimator
correspond to the list of methods in section \ref{tab03:methods}.

\begin{table}[b!]

\centering
\caption{Nodes representing modifying operations}\label{tab03:nodes}
\begin{tabular}{l c c p{0.38\textwidth}}
\toprule
\mc{\textbf{Node}\textsuperscript{1}} & \mc{\textbf{In type}\textsuperscript{2}} &
\mc{\textbf{Out type}\textsuperscript{3}} & \mc{\textbf{Operation}} \\
\midrule
cPipe       & $ens \times data$      & $out$  & Create pipeline with a preprocessor chain and a predictor \\
cPred       & $ens$                  & $out$  & Create pipeline only with a predictor \\
cData       & $featsel \times scale$ & $data$ & Create preprocessor chain with feature selector and scaler \\
cFeatSelect & $featsel$              & $data$ & Create preprocessor chain only with a feature selector \\
cScale      & $scale$                & $data$ & Create preprocessor chain only with a scaler \\
dUnion      & $data^n$               & $data$ & Create feature union in the preprocessor chain \\
\textit{ensemble} & $out^n$ & $ens$ & Insert ensemble \\
\textit{classifier} & $\emptyset$ & $out$ & Insert classifier \\
\textit{selector} & $\emptyset$ & $featsel$ & Insert feature selector \\
\textit{scaler} & $\emptyset$ & $scale$ & Insert scaler \\
\bottomrule

\multicolumn{4}{l}{\footnotesize
\textsuperscript{1}\textit{There is one specific node per ensemble, classifier
and preprocessor present}} \\

\multicolumn{4}{l}{\footnotesize
\textsuperscript{2}\textit{Variable arity is allowed (i.e. $n \in <1, max\_n>)$}} \\

\multicolumn{4}{l}{\footnotesize
\textsuperscript{3}\textit{In the last level classifier and preprocessing
can have output type $ens$ and $data$ resp.}} 

\end{tabular}

\end{table}

\subsection{Initialization} \label{sec:init}
As the initial population we grow $n$ trees where every tree has a random
height between 1 and overall maximum height. The tree is grown from root, which
is either a 'cPipe` or 'cPred` node (table \ref{tab03:nodes}). Then, nodes are
inserted into the tree according to the input type of the parent. Before the
height limit is reached (during the \emph{growing phase}), both functions and
terminals are inserted into the tree. Then, only terminals are inserted to
keep the limit.

As terminals are inserted in the growing phase as well, the tree may become
smaller than the height limit. However, if the tree were built using the full
method, it would introduce a lot of feature preprocessing methods for taller
trees. Therefore, all node types listed in table \ref{tab03:nodes} may be used
during the growing phase. Moreover, we define special terminal nodes which are
used only in the last level (\hyperref[tab03:nodes]{footnote 2}). It is
necessary to include them, as otherwise it would be impossible to finish the
tree in one level, but they would force the trees to be very short if used
during the growing phase.

\paragraph{Weighted selection}
During the node selection, we use weights to manage the probability of a node
to be chosen. The motivation is that some nodes represent lightweight methods
which have a short execution time, whereas some nodes slow down the evaluation
process, especially when present multiple times in the tree.

The process is as follows: every node is assigned to a group and each group has
a well defined weight. When selecting a node $n$ with output type $out$, we 
first determine all groups $G$ which correspond to any node with output type
$out$. Then we select a group $g$ from $G$ by a weighted random choice.
Finally, $n$ is selected by a simple random choice from $g$.

\paragraph{Variable arity}
Some nodes, e.g. ensemble nodes, may have a \emph{variable arity}. This means
that the actual arity is determined just when the node is about to be inserted
to the tree. The arity is determined by an interval which may or may not have
an upper bound. If the upper bound is not provided, it is usually limited by
a global arity limit to avoid bloat of the trees.

\paragraph{Method hyperparameters}
Every node has a list of possible values per hyperparameter associated with
it. During selection, the actual values are randomly selected from every list.
The validity is not verified in this phase, instead it is handled in the
evaluation phase \ref{genens:eval}.

\subsection{Genetic operators} \label{sec:repro}
In our system we use one type of crossover and three different types of
mutation. The crossover is the standard strongly typed subtree-swap operation.
The mutation operators will be described in more detail.

\paragraph{Subtree mutation}
As defined in section \ref{treeops}, in subtree mutation a chosen subtree is
replaced with a randomly generated tree. In our implementation we moreover
limit the height of the generated tree. For height $h$ of the subtree, the
height of the newly generated tree must be between $<1, h + \epsilon>$ for a
small value of $\epsilon$. This way we ensure that for small subtrees the new
subtree may be slightly higher and for big subtrees the overall height should
not increase too much.

\paragraph{Node swap mutation}
In this type of mutation, a randomly chosen node is replaced with a new node.
Both output and input types must match; if the new node supports variable
arity, all input types of the old node must satisfy the bounds. For this type
of mutation, any lower bound must be greater than zero.

\paragraph{Node argument mutation}
Mutates a hyperparameter of a random node --- chooses a new value from the list
of possible values. This method has many possible extensions which are more
described in section \ref{future} (future work).

\subsection{Fitness and selection} \label{sec:fitsel}

The fitness has two objectives --- evaluation score and logarithmized
evaluation time. The enviromental selection is done via NSGA-II and the 
parental selection is a tournament selection based on individual dominance and 
crowding distance. Details about method implementations are provided in 
section \ref{genens:eval}.

\section{Evaluation and performance estimation} \label{genens:eval}
In this section we elaborate the evaluation process. First we present the
pipelines and used methods in more detail. Then we present a performance
estimation method which was used to decrease the running time.

\subsection{Used machine learning methods} \label{tab03:methods}

{\footnotesize
\begin{longtable}{l l}

\caption{Used classifiers with hyperparameters}\\
	
\toprule
\multicolumn{2}{c}{\textbf{KNeighbors}} \\
\midrule

n\_neighbors & [1, 2, 5] \\
algorithm & ['auto', 'ball\_tree', 'kd\_tree', 'brute'] \\

\midrule
\multicolumn{2}{c}{\textbf{LinearSVC }} \\
\midrule

loss & [hinge,squared\_hinge] \\
penalty & [l1,l2] \\
C & [0.1,0.5,1.0,2,5,10,15] \\
tol & [0.0001,0.001,0.01] \\
 
\midrule
\multicolumn{2}{c}{\textbf{SVC }} \\
\midrule

C & [0.1,0.5,1.0,2,5,10,15] \\
gamma & [scale,0.0001,0.001,0.01,0.1,0.5] \\
tol & [0.0001,0.001,0.01] \\
 
\midrule
\multicolumn{2}{c}{\textbf{logR }} \\
\midrule

penalty & [l1,l2] \\
C & [0.1,0.5,1.0,2,5,10,15] \\
tol & [0.0001,0.001,0.01] \\
solver & [newton-cg,lbfgs,liblinear,sag,saga] \\
 
\midrule
\multicolumn{2}{c}{\textbf{Perceptron }} \\
\midrule

penalty & [None,l2,l1,elasticnet] \\
n\_iter & [1,2,5,10,100] \\
alpha & [0.0001,0.001,0.01] \\
 
\midrule
\multicolumn{2}{c}{\textbf{SGD }} \\
\midrule

penalty & [none,l2,l1,elasticnet] \\
loss & [hinge,log,modified\_huber,squared\_hinge,perceptron] \\
max\_iter & [10,100,200] \\
tol & [0.0001,0.001,0.01] \\
alpha & [0.0001,0.001,0.01] \\
l1\_ratio & [0,0.15,0.5,1] \\
epsilon & [0.01,0.05,0.1,0.5] \\
learning\_rate & [constant,optimal] \\
eta0 & [0.01,0.1,0.5] \\
power\_t & [0.1,0.5,1,2] \\
 
\midrule
\multicolumn{2}{c}{\textbf{PAC }} \\
\midrule

loss & [hinge,squared\_hinge] \\
C & [0.1,0.5,1.0,2,5,10,15] \\
 
\midrule
\multicolumn{2}{c}{\textbf{LDA }} \\*
\midrule

solver & [lsqr,eigen] \\
shrinkage & [None,auto,0.1,0.5,1.0] \\
 
\midrule
\multicolumn{2}{c}{\textbf{QDA }} \\
\midrule

reg\_param & [0.0,0.1,0.5,1] \\
tol & [0.0001,0.001,0.01] \\
 
\midrule
\multicolumn{2}{c}{\textbf{MLP }} \\
\midrule

activation & [identity,logistic,relu] \\
solver & [lbfgs,sgd,adam] \\
alpha & [0.0001,0.001,0.01] \\
learning\_rate & [constant,invscaling,adaptive] \\
tol & [0.0001,0.001,0.01] \\
max\_iter & [10,100,200] \\
learning\_rate\_init & [0.0001,0.001,0.01] \\
power\_t & [0.1,0.5,1,2] \\
momentum & [0.1,0.5,0.9] \\
hidden\_layer\_sizes & [(100,),(50,),(20,),(10,)] \\
 
\midrule
\multicolumn{2}{c}{\textbf{DT }} \\
\midrule

criterion & [gini,entropy] \\
max\_features & [0.05,0.1,0.25,0.5,0.75,1] \\
max\_depth & [1,2,5,10,15,25,50,100] \\
min\_samples\_split & [2,5,10,20] \\
min\_samples\_leaf & [1,2,5,10,20] \\
 
\midrule
\multicolumn{2}{c}{\textbf{gaussianNB  }} \\* 
\midrule\nopagebreak
\multicolumn{2}{c}{\textit{-}} \\

\midrule
\multicolumn{2}{c}{\textbf{gradBoosting }} \\
\midrule

loss & [deviance,exponential] \\
n\_estimators & [20,50,100,200] \\
subsample & [0.3,0.5,0.75,1.0] \\

 
\midrule
\multicolumn{2}{c}{\textbf{randomForest }} \\
\midrule

n\_estimators & [10,50,100,150,200] \\

 
\midrule
\multicolumn{2}{c}{\textbf{extraTrees }} \\
\midrule

n\_estimators & [10,50,100,150,200] \\
  

\bottomrule

\end{longtable}
}

{\footnotesize
\begin{longtable}{l l}

\caption{Used preprocessors with hyperparameters}\\

\midrule
\multicolumn{2}{c}{\textbf{NMF }} \\
\midrule

feat\_frac & [0.01,0.05,0.1,0.25,0.5,0.75,1] \\
solver & [cd,mu] \\
 
\midrule
\multicolumn{2}{c}{\textbf{FA }} \\
\midrule

feat\_frac & [0.01,0.05,0.1,0.25,0.5,0.75,1] \\
 
\midrule
\multicolumn{2}{c}{\textbf{FastICA }} \\
\midrule

feat\_frac & [0.01,0.05,0.1,0.25,0.5,0.75,1] \\

\midrule
\multicolumn{2}{c}{\textbf{PCA }} \\
\midrule

feat\_frac & [0.01,0.05,0.1,0.25,0.5,0.75,1] \\
whiten & [False,True] \\

\midrule
\multicolumn{2}{c}{\textbf{kBest }} \\
\midrule

feat\_frac & [0.01,0.05,0.1,0.25,0.5,0.75,1] \\
score\_func & [feature\_selection.chi2,feature\_selection.f\_classif] \\
 
\midrule
\multicolumn{2}{c}{\textbf{MaxAbsScaler  }} \\
\midrule
\multicolumn{2}{c}{\textit{-}} \\

\midrule
\multicolumn{2}{c}{\textbf{MinMaxScaler  }} \\
\midrule
\multicolumn{2}{c}{\textit{-}} \\

\midrule
\multicolumn{2}{c}{\textbf{Normalizer  }} \\
\midrule
\multicolumn{2}{c}{\textit{-}} \\

\midrule
\multicolumn{2}{c}{\textbf{StandardScaler  }} \\
\midrule
\multicolumn{2}{c}{\textit{-}} \\

\bottomrule

\end{longtable}
}


\begin{table}[b!]

\centering
\caption{Used ensembles with hyperparameters}
{ \footnotesize
\begin{tabular}{>{\itshape}l l}

\toprule
\multicolumn{2}{c}{\textbf{ada}} \\
\toprule

n\_estimators & [5, 10, 50, 100, 200] \\
algorithm & [SAMME, SAMME.R] \\

\toprule
\multicolumn{2}{c}{\textbf{bagging}} \\
\toprule

n\_estimators & [5, 10, 50, 100, 200] \\

\toprule
\multicolumn{2}{c}{\textbf{voting}} \\
\toprule

voting & [hard]\\

\bottomrule

\end{tabular}
}
\end{table}


\subsection{Scoring and sampling}
Long running time is one of the drawbacks of evolution-based systems. On
large datasets, the duration time may be too long even for a small number of
populations of small size. As such, it is necessary to decrease the fitness
evaluation time of a single individual. We use one of the performance
estimation methods mentioned in section \ref{sec:modelarch} --- evaluation on
smaller subsets of data.

For fitness evaluations, we generate samples of the original datasets (in case
of classification, the samples are stratified). The samples were either
generated per generation or per every fitness evaluation, while the latter
has proved to be more effective. A possible explanation is that if we generate 
only one sample per generation, we cannot ensure that it is representative 
enough. Thus, the individuals which score particularly well on this sample may 
not generalize well. The second approach suffers from this problem as well, but 
since we generate a sample per evaluation, every individual has a fair chance 
of getting a good sample. Possible improvements of this approach are presented 
in section \ref{future}.

As the samples are typically much smaller than the whole dataset, a simple
train-test evaluation does not generalize sufficiently. The pipeline score
is thus computed as a (stratified) k-fold cross-validation on the sample.
The predictive accuracy is the default score, but other metrics may be used
as well.
\section{Implementation}
% deap, variable arity, sklearn

% sem hyperparam genensu (cx pb,...)




% o chap 4
% exper.

% exp - evaluace - per gen, per ind, cele  10x 10 fold cv (train test?)
%    --- schovat kappu

% V patek rozmyslet experiment
% probs & wilt + small, big dataset

% popsat ten velky, jako posledni experiment
% porovnani ja vs max z openml
% popsat...
% boxplot,...
% cil je jestli to funguje...
% presne nastaveni. Vsechno. Na githubu mit to nstaveni, zkopirovat configy

% zeptat na obrazky, az to vygeneruju

% patek Conclus, Intro


% a ve ctvrtek neni prednaska
% Cil, nastaveni, vysledky tabulka obrazek jedinec; koment, co je videt, ukazali jsme co jsme chteli?
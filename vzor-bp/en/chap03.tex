\chapter{Our solution} \label{our:solution}


% uvod, co je v kapitole, nejdriv...
% obecne algorithmus bez rozzepsaneho gpcka

% Prvni cats - GPcko (zakodovani, init, operat., fitness jak se pocita (ale vic dole),...
% Druha - Sklearn strana, eval, samples,...

% Posledni- implementace, deap,...

% experimenty 4

In our solution we design an AutoML system for workflow optimization based on
developmental genetic programming. Compared to existing systems it supports 
arbitrary-sized pipelines as well as complex ensemble structures. We first
present in what way we use the genetic programming, then we describe the
pipeline evaluation process.

\section{Evolutionary optimization of pipelines}
The algorithm corresponds to the schema of a general genetic algorithm
\ref{alg:EA}. In this section we describe the necessary components of the
algorithm.

\subsection{Individual encoding}
The individual encoding is one of the most important parts of this system. With
ensembles and complex feature preprocessing methods like stacking or feature
union, most of the pipelines become in fact directed acyclic graphs (figure 
\ref{pic02:pipeline}). Therefore, we cannot directly use the simple tree-based
encoding. Instead we use the developmental GP with cellular encoding described
in section \ref{devGP}.
\begin{figure}[ht]\centering
\includegraphics[width=0.7\textwidth]{../img/pipeline-pdfa.pdf}
\caption{Schema of an example pipeline}
\label{pic02:pipeline}
\end{figure}

In our case, the embryo is an empty pipeline. To create a complex pipeline, we
modify it by inserting steps into it. All modification operations are listed in
table \ref{tab03:nodes}.

\begin{figure}[ht]\centering
    \subfloat[Tree encoding]{{\includegraphics[width=0.25\textwidth]{../img/ada.png} }}%
    \qquad
    \subfloat[Encoded pipeline]{{\includegraphics[width=0.5\textwidth]{../img/ada-pdfa.pdf} }}%
    \caption{An example pipeline encoded to a tree individual}%
    \label{pic:pipeencoding}%
\end{figure}

The process can be demonstrated on figure
\ref{pic:pipeencoding}. The root of the tree represents the embryo which will
be modified by subsequent operations. In this case, the left subtree modifies
the ensemble structure whereas the right subtree modifies the feature
preprocessor chain. The pipeline contains only one preprocessor, hence the
right subtree is terminated by the corresponding node. The left son can be
either an ensemble or a simple method. Here it is the AdaBoost ensemble which
has one base classifier. The subestimator is again a pipeline, which is composed
of a MinMaxScaler and Stochastic gradient descent classifier.

Table \ref{tab03:nodes} lists the nodes used in the current implementation of
our system. Input type is defined as a cartesian product of types, output type
is a single type; terminals define only the output type. Output types of child
nodes must match the input type of parent node. The list is extensible, it is
possible to add a definion of similar nodes, e.g. a different ensemble flavour
like stacking.

% TODO should I describe node types?
\begin{table}[b!]

\centering
\begin{tabular}{l c c p{0.38\textwidth}}
\toprule
\mc{\textbf{Node}\textsuperscript{1}} & \mc{\textbf{In type}} &
\mc{\textbf{Out type}\textsuperscript{2}} & \mc{\textbf{Operation}} \\
\midrule
cPipe       & $ens \times data$      & $out$  & Create pipeline with a preprocessor chain and a predictor \\
cPred       & $ens$                  & $out$  & Create pipeline only with a predictor \\
cData       & $featsel \times scale$ & $data$ & Create preprocessor chain with feature selector and scaler \\
cFeatSelect & $featsel$              & $data$ & Create preprocessor chain only with a feature selector \\
cScale      & $scale$                & $data$ & Create preprocessor chain only with a scaler \\
dUnion      & $data^n$               & $data$ & Create feature union in the preprocessor chain \\
\textit{ensemble} & $out^n$ & $ens$ & Insert ensemble \\
\textit{classifier} & $\emptyset$ & $out$ & Insert classifier \\
\textit{selector} & $\emptyset$ & $featsel$ & Insert feature selector \\
\textit{scaler} & $\emptyset$ & $scale$ & Insert scaler \\
\bottomrule

\multicolumn{4}{l}{\footnotesize
\textsuperscript{1}\textit{There is one specific node per ensemble, classifier
and preprocessor present}} \\
\multicolumn{4}{l}{\footnotesize
\textsuperscript{2}\textit{In the last level classifier and preprocessing
can have output type $ens$ and $data$ resp.}} 

\end{tabular}
\caption{Nodes representing modifying operations}\label{tab03:nodes}

\end{table}

\subsection{Initialization}
As the initial population we grow $n$ trees where every tree has a random
height between 1 and overall maximum height. The tree is grown from root, which
is either a 'cPipe` or 'cPred` node (table \ref{tab03:nodes}). Then, nodes are
inserted into the tree according to the input type of the parent. Before the
height limit is reached (during the \emph{growing phase}), both functions and
terminals are inserted into the tree. Then, only terminals are inserted to
keep the limit.

As terminals are inserted in the growing phase as well, the tree may become
smaller than the height limit. However, if the tree were built using the full
method, it would introduce a lot of feature preprocessing methods for taller
trees. Therefore, all node types listed in table \ref{tab03:nodes} may be used
during the growing phase. Moreover, we define special terminal nodes which are
used only in the last level (\hyperref[tab03:nodes]{footnote 2}). It is
necessary to include them, as otherwise it would be impossible to finish the
tree in one level, but they would force the trees to be very short if used
during the growing phase.

In addition to this process, we use weights to manage the probability of a node
to be chosen. The motivation is that some nodes represent lightweight methods
which have a short execution time, whereas some nodes slow down the evaluation
process, especially when present multiple times in the tree.

The process is as follows: every node is assigned to a group and each group has
a well defined weight. When selecting a node $n$ with output type $out$, we 
first determine all groups $G$ which correspond to any node with output type
$out$. Then we select a group $g$ from $G$ by a weighted random choice.
Finally, $n$ is selected by a simple random choice from $g$.

\subsection{Genetic operators}

\subsection{Fitness and selection}

\section{Evaluation and performance estimation} \label{sec:eval}

\section{Implementation}
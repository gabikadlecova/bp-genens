\chapter{Preliminaries}

%An~example citation: \cite{Andel07}
\textit{What we will talk about, theory}

\section{Machine learning}
The field of machine learning encompasses a broad range of algorithms and
statistical methods for data processing. In his book on machine learning,
Flach provides the following general definition:

\blockquote{Machine learning is the systematic study of algorithms and systems
that improve their knowledge or performance with experience.
(\cite{Flach:2012:MLA:2490546})} % page number? (3)
%
% Here a detailed definition
%
The knowledge of a system is gained through learning from \emph{experience}.
This procedure is referred to as the \emph{training} phase. In this process, 
the algorithm adjusts its parameters according to the nature of training data.
The result of the process is a prediction function which depends on learned
parameters. By applying this function on previously unseen data, denominated 
as the \emph{testing} data, we obtain the output of the algorithm. The 
result is then evaluated to determine the performance of the method. 
(\cite{Bishop:2006:PRM:1162264}) The character of the learning process
varies with different machine learning problems. There are three main classes 
of tasks: \emph{supervised}, \emph{unsupervised} and \emph{reinforcement} 
learning.
% This should be improved

In case of supervised learning, the training data is a set of labelled examples
and the task is to predict labels of previously unseen data. The field is 
further subdivided into two groups. If the labels are elements of a finite 
number of discrete categories, the problem is called \emph{classification}. 
The continuous case is then called \emph{regression}.
% This is Bishop as well... TODO cite pages

In contrast to supervised learning, in unsupervised learning the training data 
is unlabelled. The key task is therefore to divide the data into groups of 
similar examples.

The task of reinforcement learning is to find suitable actions as to maximize 
a reward. The training data is typically some history of previous actions 
and corresponding rewards.

It is important to note that good performance on training data does not ensure 
just as good performance on new data. Sometimes the model performs 
exceptionally well on training data, but fares much worse on testing data. 
This behaviour is called \emph{overfitting} and usually occurs when 
unnecessarily subtle details of the data are learned. The opposite concept 
is called \emph{generalisation}, which is the ability to perform well on 
different types of testing data.

A related term is the so-called `\emph{bias-variance dilemma}'. A 
low-complexity model will not overfit, but a lot of errors will be made, thus 
introducing a certain bias from the correct output. On the other hand, by 
increasing the number of model parameters, it will highly depend on training 
data. Then, with small changes in data there will be a high variance in 
output. A balance can rarely be achieved in practice, hence it is often 
necessary to choose the side that is less harmful to the task. Another option
is to use some of the ensemble methods described in \ref{ensemble}. For
example, \hyperref[bagging]{bagging} is a method of variance reduction, while
\hyperref[boosting]{boosting} noticeably reduces the bias. More on this topic
can be found in Flach's book. (\citet[p.~93--94,~338]{Flach:2012:MLA:2490546})

% here more on supervised learning, hypotheses,...

\subsection{Model ensembles} \label{ensemble}
% Mention netflix like they did in the paper?
% Find sources where they mention successful applications
Model ensembles are powerful learning techniques that combine simpler models 
to achieve better results. They are widely used in practice, specific examples
can be found in the review of \cite{Rokach:2009:TCE:1609202.1609436}.

The rationale behind ensembles is also of theoretical character, namely from
statistics and from computational learning theory. In statistics, a general
idea is to average measurements to get more stable and reliable results.
Here, the models are trained on data samples or feature subsets and the results
are then combined into a final hypothesis.
% Flach

% Variant A, possibly inacurrate
The computational learning theory defines the term \emph{learnability},
which describes whether a model outputs a correct hypothesis by learning on
random sets of instances from a unknown distribution. A \emph{strong learner}
is a model which outputs most of the time a correct hypothesis. It is not
required that it always produces a hypothesis with error equal to zero, as 
the set of chosen instances might be atypical or not representative enough.
Similarly, a \emph{weak learner} is then a model which outputs most of the time
a hypothesis, which is slightly better than random guessing (i.e. has a
success rate over 0.5). A more detailed elaboration of the learnability
theory is beyond the scope of this work and can be found in books 
of \citep{Flach:2012:MLA:2490546} and \citep{Mitchell:1997:ML:541177}.

% Variant B, possibly tedious to read
%The computational learning theory defines the term \emph{learnability},
%which describes whether a concept language can be reliably learnt. A concept is
%defined as a logical expression which describes a set of instances. A 
%hypothesis space is then the space of possible concepts.

%Having a model $M$ and a concept language $C$, $C$ is (strongly)
%\emph{PAC-learnable} by $M$ if the model outputs most of the time a hypothesis 
%with a very small error. The model is not required to be correct every time, 
%as the set of chosen instances described by the language might be atypical 
%or not representative enough.  We can also define the \emph{weak learnability} 
%of $C$ by $M$ where $M$ outputs a hypothesis which is slightly better than 
%random guessing (i.e. success rate over 0.5). 

The assumption of strong learnability may appear to be quite strict when 
compared to the weak learnability, as the model must output a correct 
hypothesis on almost all example sets. However, Shapire proved that a model 
is weakly learnable if and only if it is strongly learnable. 
\citep{Schapire:1990:SWL:83637.83645} This was proven in a constructive manner
by iteratively correcting the errors of the hypotheses, thus \emph{boosting} 
the model. The boosting method has directly inspired one of the most successful
ensemble methods --- the award winning AdaBoost.
\cite{Freund:1996:ENB:3091696.3091715, Freund:1997:DGO:261540.261549}

In the following sections, we will present some of the most used ensemble 
methods.
\phantomsection
\paragraph{Bagging} \label{bagging}
\emph{Bootstrap aggregating}, usually abbreviated to bagging, is a highly
effective ensemble method. First, $n$ samples are independently taken from 
the original dataset with replacement. This is referred to as 
\emph{bootstrapping}. Then, we use the samples to train an ensemble of $n$
different models. It can be then used to generate predictions, which can be
then \emph{aggregated} by voting or averaging.

This method takes advantage of the statistical stability described at the
beginning of this section. As the examples are drawn with replacement, there
will be some instances missing in every sample. Thus, we introduce diversity
between the ensemble models. \citep[331]{Flach:2012:MLA:2490546}

\phantomsection
\paragraph{Boosting} \label{boosting}
The above-mentioned boosting technique uses a different approach to model
combining. Before the learning process starts, we add weights to the training 
examples (the base-learner must support weighting). Then, we learn the model
on the modified training set, which produces a set of misclassified instances
along with the (weighted) training error. We then adjust the weights in such a
way that weights of the correctly classified examples decrease and those of
the incorrectly classified examples increase. Therefore, when we continue and
learn a new model on the data with changed weights, it will concentrate more 
on the problematic instances.

The algorithm stops after a fixed number of iterations or when the weighted
training error increases over $0.5$ --- which is when the algorithm stops
improving. The resulting prediction is again an average of all model
predictions, but with putting more weight on models with a lower training
error. \citep[335]{Flach:2012:MLA:2490546}

\section{AutoML}
When using machine learning in practice, it is not always evident which model
is suitable for a particular problem. Moreover, there is a vast number of model
and parameter combinations to choose from, not to mention model ensembles. 
AutoML, aims to solve these problems by automatizing the process of model 
selection.


...section model architecture: NAS, ensembles, examples in chapter 2.
% problemy - jak to skladat dohromady,...
% jak slozit model vs jak vybrat model
% doporucovani hyperparam.

\subsection{Metalearning}
The metalearning, also known as `learning to learn' \ldots

Just as the traditional learning --- here referred to as \emph{base-learning}
--- metalearning improves with experience. The difference lies in the learning
process. While base-learning comprises of a single run on a specific task,
metalearning may include several runs or many different tasks.

In the research area of model recommendation, the training data is most
frequently a history of previous runs. A suitable model is then chosen by
examining which models were successful on similar tasks. As the problem data
may differ significantly, the task cannot be usually compared directly.
Therefore, we need to accumulate some kind of \emph{metaknowledge} which
can be extracted from substantially different data.

\textit{Describe meta-data, meta-features, EVA approach.}

\section{Evolutionary computing} \label{ea}
Evolutionary computing is a heuristic optimization method inspired by 
Charles Darwin's theory of \emph{natural selection}. \cite{darwin} In 
a population, individuals with the best characteristics are most likely
to reproduce, thus passing the traits to the offspring. As the 
evolution is repeated over several generations, the most advantageous traits 
predominate. This phenomenon is also called `survival of the fittest'.

In an evolutionary algorithm, the goal is to find the ``best'' solution 
to the given problem by optimizing a \emph{objective function}. The term
`population' refers to a set of solutions encoded as chromosomes which 
represent the defining features of a particular solution. This corresponds
to the genotype--phenotype relationship from genetics. The `natural selection'
can be then understood as a stochastic search through the space of possible 
chromosome values. 
(\cite{Engelbrecht:2007:CII:1557464})

\textit{exploration vs exploitation...}

% Maybe a better section name
\subsection{Evolutionary algorithms in detail}
As can be seen in algorithm \ref{alg:EA}, a genetic algorithm should 
define a suitable \emph{selection} method, \emph{mutation} and/or 
\emph{crossover} operators and a \emph{fitness function}.
The algorithm terminates when some \emph{stopping condition} is met. 

\paragraph{Selection}
The selection may be divided into two steps. The first is the 
\emph{parent selection}, also called \emph{mating selection} which is applied 
on line \ref{parent:sel}, and the second is called
\emph{environmental selection} or \emph{recombination}.
Sometimes the latter is omitted, as the selection can be limited to copying 
all offspring to the new population.

The purpose of the parent selection is to select individuals for the
mating process. Usually, it is a probabilistic process with `better'
individuals being more likely to be selected. The worse individuals have some
chance to be selected as well for the sake of maintaining diversity in the
population. An example of the mating selection is the
\emph{tournament selection}, where individuals `compete' in rounds and the
overall winner is selected. A round is won by an individual, if it has a 
greater fitness value.

The environmental selection is used to create a new population. Unlike the
mating selection which is a stochastic process, replacement is usually
deterministic. Individuals with a higher fitness are usually preferred as in
the first type of selection, but the decision may take into account the age
of the individuals. As such, it is possible to include or not to include the
parents along with the offspring. A popular option is to directly choose
a small number of the most successful individuals. This method is called
\emph{elitism} \citep{Eiben:2015:IEC:2810085}.
An example of an elitist selection algorithm is NSGA-II, which
is described in \ref{nsgaii}.
% TODO cite pages directly, when citing from a single source for several
% paragraphs (because it's good)

\paragraph{Crossover and mutation}
The crossover and mutation (also called reproduction operators
\citep{Engelbrecht:2007:CII:1557464}) are genetic operators that create new
individuals. Both operations are highly dependent on problem encoding, as they
alter the structure of the genomes of individuals. In the schema of the 
\hyperref[evolutionary algorithm]{alg:EA}, reproduction operators are applied
on parents selected by the mating selection, as can be seen on lines
\ref{crossover} and \ref{mutation}.

During the crossover, the genetic information of the parents is combined into
one or more children. Most usually, two parents are used to produce two
offspring, though the counts may differ in some special types of evolutionary
algorithms. Ideally, if we have two parents with different but nevertheless
`good' features, the offspring receives both of them. 

The mutation is a stochastic operator which is used to introduce diversity into
the population.

\paragraph{Stopping criteria}
Some commonly used stopping criteria, as listed by Engelbrecht, are for example 
a limit on the number of generations, a objective function threshold or
termination after no improvement is observed.
(\citep{Engelbrecht:2007:CII:1557464})

% Ze mame ruzny metody, co to delaj ruzne, konkretni popiseme v nasem reseni
% jednobodovy apod nepsat, spis to trochu uzavrit

% ocislovat radky, odkazovat na ne, zakladni prvky jsou
\begin{algorithm}
\DontPrintSemicolon
  \KwData{population size $k$, stopping condition $c$, 
          crossover probability $p_{cx}$ and mutation probability $p_{mut}$}
  \KwResult{evolved individuals}
  \;
  $P(0) \longleftarrow$ population of size $k$

  \While{$c$ is not met}{
      \For{individual $ind$} {
         compute fitness $f(ind)$
      }
      \For{i in \Range{$k/2$}} {
         $i_1, i_2 \longleftarrow$ select two individuals \label{parent:sel}

         \If{$p_{cx}$} {
            $crossover(i_1, i_2)$ \label{crossover}
         }
         
         \If{$p_{mut}$ for $k=1,2$} {
            $mutation(i_k)$ \label{mutation}
         }
      }
      \;
      $P(n+1) \longleftarrow$ select $k$ individuals from $P(n)$ \label{envir:sel}
  }
  \;
  return $P(c)$  
\caption{Evolutionary algorithm\label{alg:EA}}
\end{algorithm}

The advantage of genetic algorithms is such that there are potentially 
many different solutions present in every population. With well defined 
selection and fitness, the algorithm performs a multi-directional search. 
In comparison with other directed search methods, this proves to be a more 
robust approach. (\cite{Michalewicz:1996:GAD:229930}, 
\cite{Mitchell:1997:ML:541177}) % Mitchell page 260

\subsection{Multi-objective optimization}
In many problems, the quality of the solution depends on more than one
objective function. With this, it is much harder to say whether one solution
is strictly better than another. Multi-objective optimization (MOO) formally
describes this class of problems. We first define the general terminology
and then present MOO in evolutionary computation.

In an optimization problem, the task is to maximize or minimize the objective
function $f(x)$, where $x$ is a vector from the search space. The problem may
also be restricted by constraints in the form of equalities and inequalities.
In multi-objective optimization, the setting remains the same, but the
objective function changes to an \emph{objective vector} ---
for objective functions $f_i(x), i = 1,\ldots,k$, the objective vector is 
defined as $f(x)~=~(f_1(x), f_2(x), \ldots, f_k(x))$.

Although the problem setting is similar, the meaning of optimality needs to 
be redefined. For once, given a pair of solutions, one solution may be better 
than the other solution in one objective function and worse in another. For 
this purpose, we need the following definition.

\begin{definition}[Domination]
In a minimization problem, a vector $x_1$ dominates a vector $x_2$ if and 
only if 
\begin{compactitem}
\item $\forall i=1,\ldots,k: f_i(x_1) \leq f_i(x_2)$, and
\item $\exists j=1,\ldots,k: f_j(x_1) < f_j(x_2)$
\end{compactitem}
\end{definition}

As it is possible to have solutions where neither one dominates the other,
it is impossible to determine one optimal solution. Hence we define the 
\emph{Pareto-optimality}.

\begin{definition}[Pareto-optimality]
A vector $x_1$ is said to be \emph{Pareto-optimal}, if there is no other vector
$x_2$ that dominates it. The \emph{Pareto-optimal set} $P^*$ is the set of all
non-dominated solutions. Finally, the \emph{Pareto-optimal front} (Pareto front)
is defined as
$PF^* =\{\, f=(f_1(x^*),\ldots f_k(x^*)) \mid x^* \in P^* ) \,\}$.
\end{definition}

\citep[p.~551-561,~569--573]{Engelbrecht:2007:CII:1557464}

If we use evolutionary computation to solve multi-objective problems, the
algorithm needs to be modified. As not every individuals are directly
comparable, we cannot use the selection operators defined in
\ref{ea}. As such, there are various approaches on how to solve this problem,
which can be divided into three groups:

\begin{compactitem}
\item Weighted aggregation --- define a single objective function as a weighted
sum of sub-objectives and proceed with standard evolutionary algorithm
\item Population-based non-Pareto solutions --- works with the sub-objectives,
but does not use the dominance
\item Pareto-based solutions --- tries to approximate the Pareto front
\end{compactitem}

From these three groups, we describe in more detail one Pareto-based algorithm.
More examples are presented in 
\cite[p.~170-173]{Engelbrecht:2007:CII:1557464}.
The algorithm is called Nondominated sorting genetic algorithm (NSGA). It is a
\emph{ranking} selection, which means that individuals are sorted by their
fitness values and the selection is performed with regard to the ordering.

To compute the fitness, the individuals are divided into non-dominated fronts.
This is done by finding a Pareto front of a subpopulation, assigning a
front number to its individuals and removing the from the subpopulation.
The process is repeated with front numbers increasing until no unassigned
individuals remain. Every front then obtains a dummy fitness value, where
the fitness of a front $F(n)$ is better than the fitness of $F(n+1)$. Moreover,
for every individual the value is divided by a \emph{niching} factor (while
keeping the fitness inequality). The 
niching factor is defined as
$$N(i)=\sum_{j\neq i}{S(d((i,j))}$$ where 
\begin{equation}
    S(d(i,j))=
    \begin{cases}
      1 - (\dfrac{d(i,j)}{\sigma_{share}})^2, & \text{if}\ d(i,j) < \sigma_{share} \\
      0, & \text{otherwise.}
    \end{cases}
\end{equation}
This is the definition from the original article of
\cite{Srinivas:1994:MOU:1326668.1326671}, but it can be computed in a different
manner, for example as the count of individuals closer than $\sigma_{share}$
\citep{Engelbrecht:2007:CII:1557464}.

\label{nsgaii}
As the algorithm has some drawbacks, like dependence on $\sigma_{share}$,
a very high computational complexity and lack of elitism, the authors of
NSGA have proposed an improved variant called NSGA-II. This algorithm not only
adresses the above-mentioned problems, it also outperforms other elitist
algorithms \citep{Deb:2002:FEM:2221359.2221582}.


\section{Genetic programming}
% zkratky Genetic progr, GP... uvest...
% mozna seznam zkratek...
In this section, we present a subfield of evolutionary computing --- 
the genetic programming --- where the population is a set of computer 
programs. The aim of this technique is to evolve programs that provide 
a good solution to the given problem. There are various approaches in means 
of how to represent the individuals and what kind of genetic operators to 
use. The fitness is computed by running the program and comparing the result 
with the desired output. \cite{Poli:2008:FGG:1796422}

\textit{Mention typed GP here}
\subsection{Tree-based genetic programming}
The individuals are most frequently represented in the form of 
\emph{syntax trees}. Inner nodes of the tree are functions, whereas leaves 
are constants and variables. The initialization step is thus very important, as
there are many different ways how to design trees. Also, specialized genetic
operators need to be designed.

\paragraph{Initialization}
During the initialization, both functions and constants are selected from a set
of possible nodes which is provided as input to the algorithm. As was mentioned,
there are various methods of initialization. We well present two methods that
are among the simplest and most used ones --- \emph{grow} and \emph{full}.
% set... cite Koza's book

In both cases, nodes are inserted to the tree up to a certain height limit.
The two methods differ only in the way how nodes are selected. The grow method
allows to select both functions and terminals before the limit is reached;
afterwards, only terminals can be inserted. The full method restricts the
selection only to functions on all levels but the last one, thus generating 
a full tree. Leaves are then
chosen from the terminal set like in the previous approach.

The drawback of the full method is that all trees are very similar. On the
contrary, the grow method generates a wide range of sizes and shapes, but the
number of nodes in a tree might be too small. Because of that, a method called
\emph{ramped half-and-half} is often used in practice. It combines both 
of the presented methods; half of the population is generated using the full
method, the other one via grow method. Also, instead of one height limit, a
range of values is used to introduce more diversity.

\paragraph{Genetic operators}
The most common type of crossover is \emph{subtree crossover} of two
individuals. A random node --- the crossover point ---
is selected in each individual independently. Then, subtrees corresponding
to the points are exchanged between them.

Similarly, the most used mutation technique is \emph{subtree mutation}.
Just like in subtree crossover, a mutation point is randomly chosen.
Afterwards, the corresponding subtree is entirely replaced by a new randomly
generated tree.
% Genetic operators? Pictures?

\subsection{Developmental genetic programming}

\section{Workflows}
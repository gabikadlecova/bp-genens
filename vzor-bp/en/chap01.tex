\chapter{Preliminaries}

%An~example citation: \cite{Andel07}
\textit{What we will talk about, theory}

\section{Machine learning}
The field of machine learning encompasses a broad range of algorithms and
statistical methods for data processing. In his book on machine learning,
Flach provides the following general definition:

\blockquote{Machine learning is the systematic study of algorithms and systems
that improve their knowledge or performance with experience.
(\cite{Flach:2012:MLA:2490546})} % page number? (3)
%
% Here a detailed definition
%
The knowledge of a system is gained through learning from \emph{experience}.
This procedure is referred to as the \emph{training} phase. In this process, 
the algorithm adjusts its parameters according to the nature of training data.
The result of the process is a prediction function, which depends on learned
parameters. By applying this function on previously unseen data, denominated 
as the \emph{testing} data, we obtain the output of the algorithm. The 
result is then evaluated to determine the performance of the method. 
(\cite{Bishop:2006:PRM:1162264}) The character of the learning process
varies with different machine learning problems. There are three main classes 
of tasks: \emph{supervised}, \emph{unsupervised} and \emph{reinforcement} 
learning.
% This should be improved

In case of supervised learning, the training data is a set of labelled examples
and the task is to predict labels of previously unseen data. The field is 
further subdivided into two groups. If the labels are elements of a finite 
number of discrete categories, the problem is called \emph{classification}. 
The continuous case is then called \emph{regression}.
% This is Bishop as well... TODO cite pages

In contrast to supervised learning, in unsupervised learning the training data 
is unlabelled. The key task is therefore to divide the data into groups of 
similar examples.

The task of reinforcement learning is to find suitable actions as to maximize 
a reward. The training data is typically some history of previous actions 
and corresponding rewards.

It is important to note that good performance on training data does not ensure 
just as good performance on new data. Sometimes the model performs 
exceptionally well on training data, but fares much worse on testing data. 
This behaviour is called \emph{overfitting} and usually occurs when 
unnecessarily subtle details of the data are learned. The opposite concept 
is called \emph{generalisation}, which is the ability to perform well on 
different types of testing data.

A related term is the so-called `\emph{bias-variance dilemma}'. A 
low-complexity model will not overfit, but a lot of errors will be made, thus 
introducing a certain bias from the correct output. On the other hand, by 
increasing the number of model parameters, it will highly depend on training 
data. Then, with small changes in data there will be a high variance in 
output. A balance can rarely be achieved in practice, hence it is often 
necessary to choose the side which is less harmful to the task. Another option
is to use some of the ensemble methods described in \ref{ensemble}. For
example, \hyperref[bagging]{bagging} is a method of variance reduction, while
\hyperref[boosting]{boosting} noticeably reduces the bias. More on this topic
can be found in Flach's book. (\citet[p.~93--94,~338]{Flach:2012:MLA:2490546})

% here more on supervised learning, hypotheses,...

\subsection{Model ensembles} \label{ensemble}
% Mention netflix like they did in the paper?
% Find sources where they mention successful applications
Model ensembles are powerful learning techniques that combine simpler models 
to achieve better results. They are widely used in practice, specific examples
can be found in the review of \cite{Rokach:2009:TCE:1609202.1609436}.

The rationale behind ensembles is also of theoretical character, namely from
statistics and from computational learning theory. In statistics, a general
idea is to average measurements to get more stable and reliable results.
Here, the models are trained on data samples or feature subsets and the results
are then combined into a final hypothesis.
% Flach

% Variant A, possibly inacurrate
The computational learning theory defines the term \emph{learnability},
which describes whether a model outputs a correct hypothesis by learning on
random sets of instances from a unknown distribution. A \emph{strong learner}
is a model which outputs most of the time a correct hypothesis. It is not
required that it always produces a hypothesis with error equal to zero, as 
the set of chosen instances might be atypical or not representative enough.
Similarly, a \emph{weak learner} is then a model which outputs most of the time
a hypothesis, which is slightly better than random guessing (i.e. has a
success rate over 0.5). A more detailed elaboration of the learnability
theory is beyond the scope of this work and can be found in books 
of \citep{Flach:2012:MLA:2490546} and \citep{Mitchell:1997:ML:541177}.

% Variant B, possibly tedious to read
%The computational learning theory defines the term \emph{learnability},
%which describes whether a concept language can be reliably learnt. A concept is
%defined as a logical expression which describes a set of instances. A 
%hypothesis space is then the space of possible concepts.

%Having a model $M$ and a concept language $C$, $C$ is (strongly)
%\emph{PAC-learnable} by $M$ if the model outputs most of the time a hypothesis 
%with a very small error. The model is not required to be correct every time, 
%as the set of chosen instances described by the language might be atypical 
%or not representative enough.  We can also define the \emph{weak learnability} 
%of $C$ by $M$ where $M$ outputs a hypothesis which is slightly better than 
%random guessing (i.e. success rate over 0.5). 

The assumption of strong learnability may appear to be quite strict when 
compared to the weak learnability, as the model must output a correct 
hypothesis on almost all example sets. However, Shapire proved that a model 
is weakly learnable if and only if it is strongly learnable. 
\citep{Schapire:1990:SWL:83637.83645} This was proven in a constructive manner
by iteratively correcting the errors of the hypotheses, thus \emph{boosting} 
the model. The boosting method has directly inspired one of the most successful
ensemble methods --- the award winning AdaBoost.
\cite{Freund:1996:ENB:3091696.3091715, Freund:1997:DGO:261540.261549}

In the following sections, we will present some of the most used ensemble 
methods.
\phantomsection
\paragraph{Bagging} \label{bagging}
\emph{Bootstrap aggregating}, usually abbreviated to bagging, is a highly
effective ensemble method. First, $n$ samples are independently taken from 
the original dataset with replacement. This is referred to as 
\emph{bootstrapping}. Then, we use the samples to train an ensemble of $n$
different models. It can be then used to generate predictions, which can be
then \emph{aggregated} by voting or averaging.

This method takes advantage of the statistical stability described at the
beginning of this section. As the examples are drawn with replacement, there
will be some instances missing in every sample. Thus, we introduce diversity
between the ensemble models. \citep[331]{Flach:2012:MLA:2490546}

\phantomsection
\paragraph{Boosting} \label{boosting}
The above-mentioned boosting technique uses a different approach to model
combining. Before the learning process starts, we add weights to the training 
examples (the base-learner must support weighting). Then, we learn the model
on the modified training set, which produces a set of misclassified instances
along with the (weighted) training error. We then adjust the weights in such a
way that weights of the correctly classified examples decrease and those of
the incorrectly classified examples increase. Therefore, when we continue and
learn a new model on the data with changed weights, it will concentrate more 
on the problematic instances.

The algorithm stops after a fixed number of iterations or when the weighted
training error increases over $0.5$ --- which is when the algorithm stops
improving. The resulting prediction is again an average of all model
predictions, but with putting more weight on models with a lower training
error. \citep[335]{Flach:2012:MLA:2490546}

\section{AutoML}
When using machine learning in practice, it is not always evident which model
is suitable for a particular problem. Moreover, there is a vast number of model
and parameter combinations to choose from, not to mention model ensembles. 
AutoML, aims to solve these problems by automatizing the process of model 
selection.


...section model architecture: NAS, ensembles, examples in chapter 2.
% problemy - jak to skladat dohromady,...
% jak slozit model vs jak vybrat model
% doporucovani hyperparam.

\subsection{Metalearning}
The metalearning, also known as `learning to learn' \ldots

Just as the traditional learning --- here referred to as \emph{base-learning}
--- metalearning improves with experience. The difference lies in the learning
process. While base-learning comprises of a single run on a specific task,
metalearning may include several runs or many different tasks.

In the research area of model recommendation, the training data is most
frequently a history of previous runs. A suitable model is then chosen by
examining which models were successful on similar tasks. As the problem data
may differ significantly, the task cannot be usually compared directly.
Therefore, we need to accumulate some kind of \emph{metaknowledge} which
can be extracted from substantially different data.

\textit{Describe meta-data, meta-features, EVA approach.}

\section{Evolutionary computing}
Evolutionary computing is a heuristic optimization method inspired by 
Charles Darwin's theory of \emph{natural selection}. \cite{darwin} In 
a population, individuals with the best characteristics are most likely
to reproduce, thus passing the traits to the offspring. As the evolution 
is repeated over several generations, the most advantageous traits 
predominate. This phenomenon is also called `survival of the fittest'. 
% Similar to Engelbrecht... So maybe rewrite it
% optimalizator,...

In an evolutionary algorithm, the goal is to find the ``best'' solution 
to the given problem by optimizing a \emph{objective function}. The term
`population' refers to a set of solutions encoded as chromosomes which 
represent the defining features of a particular solution. This corresponds
to the genotype--phenotype relationship from genetics. The `natural selection'
can be then understood as a stochastic search through the space of possible 
chromosome values. 
(\cite{Engelbrecht:2007:CII:1557464}) 
% tohle je skoro doslova, jak moc to musim menit? 
% Kdyz je to v podstate definice

% genotyp fenotyp

As can be seen in algorithm \ref{alg:EA}, a genetic algorithm should 
define a suitable \emph{selection} method, \emph{mutation} and/or 
\emph{crossover} operators and a \emph{fitness function}.
The algorithm terminates when some \emph{stopping condition} is met. 
Some commonly used criteria, as listed by Engelbrecht, are for example 
a limit on the number of generations, a objective function threshold or
termination after no improvement is observed.
(\citep{Engelbrecht:2007:CII:1557464})

% Ze mame ruzny metody, co to delaj ruzne, konkretni popiseme v nasem reseni
% jednobodovy apod nepsat, spis to trochu uzavrit

% ocislovat radky, odkazovat na ne, zakladni prvky jsou
\begin{algorithm}
\DontPrintSemicolon
  \KwData{population size $n$, stopping condition $c$, 
          crossover probability $p_{cx}$ and mutation probability $p_{mut}$}
  \KwResult{evolved individuals}
  \;
  $P(0) \longleftarrow$ population of size $n$

  \While{$c$ is not met}{
      \For{individual $ind$} {
         compute fitness $f(ind)$
      }
      \For{i in \Range{$n/2$}} {
         $i_1, i_2 \longleftarrow$ select two individuals
         
         \If{$p_{cx}$} {
            perform crossover
         }
         
         \If{$p_{mut}$} {
            perform mutation
         
         }
      }
      $P(n+1) \longleftarrow$ select individuals from $P(n)$   
  }
  \;
  return $P(c)$  
\caption{Evolutionary algorithm\label{alg:EA}}
\end{algorithm}

The advantage of genetic algorithms is such that there are potentially 
many different solutions present in every population. With well defined 
selection and fitness, the algorithm performs a multi-directional search. 
In comparison with other directed search methods, this proves to be a more 
robust approach. (\cite{Michalewicz:1996:GAD:229930}, 
\cite{Mitchell:1997:ML:541177}) % Mitchell page 260

\subsection{Multi-objective optimization}
In many problems, the quality of the solution depends on more than one
objective function. With this, it is much harder to say whether one solution
is strictly better than another. Multi-objective optimization formally
describes this class of problems.

In an optimization problem, the task is to maximize or minimize the objective
function $f(x)$, where $x$ is a vector from the search space. The problem may
also be restricted by constraints in the form of equalities and inequalities.
In multi-objective optimization, the setting remains the same, but the
objective function changes to an \emph{objective vector} ---
for objective functions $f_i(x), i = 1,\ldots,k$, the objective vector is 
defined as $f(x)~=~(f_1(x), f_2(x), \ldots, f_k(x))$.



\section{Genetic programming}
% zkratky Genetic progr, GP... uvest...
% mozna seznam zkratek...
In this section, we present a subfield of evolutionary computing --- 
the genetic programming --- where the population is a set of computer 
programs. The aim of this technique is to evolve programs which provide 
a good solution to the given problem. There are various approaches in means 
of how to represent the individuals and what kind of genetic operators to 
use. The fitness is computed by running the program and comparing the result 
with the desired output. \cite{Poli:2008:FGG:1796422}

\textit{Mention typed GP here}
\subsection{Tree-based genetic programming}
The individuals are most frequently represented in the form of 
\emph{syntax trees}. Inner nodes of the tree are functions, whereas leaves 
are constants and variables. The initialization step is thus very important, as
there are many different ways how to design trees. Also, specialized genetic
operators need to be designed.

\paragraph{Initialization}
During the initialization, both functions and constants are selected from a set
of possible nodes which is provided as input to the algorithm. As was mentioned,
there are various methods of initialization. We well present two methods which
are among the simplest and most used ones --- \emph{grow} and \emph{full}.
% set... cite Koza's book

In both cases, nodes are inserted to the tree up to a certain height limit.
The two methods differ only in the way how nodes are selected. The grow method
allows to select both functions and terminals before the limit is reached;
afterwards, only terminals can be inserted. The full method restricts the
selection only to functions on all levels but the last one, thus generating 
a full tree. Leaves are then
chosen from the terminal set like in the previous approach.

The drawback of the full method is that all trees are very similar. On the
contrary, the grow method generates a wide range of sizes and shapes, but the
number of nodes in a tree might be too small. Because of that, a method called
\emph{ramped half-and-half} is often used in practice. It combines both 
of the presented methods; half of the population is generated using the full
method, the other one via grow method. Also, instead of one height limit, a
range of values is used to introduce more diversity.

\paragraph{Genetic operators}
The most common type of crossover is \emph{subtree crossover} of two
individuals. A random node --- the crossover point ---
is selected in each individual independently. Then, subtrees corresponding
to the points are exchanged between them.

Similarly, the most used mutation technique is \emph{subtree mutation}.
Just like in subtree crossover, a mutation point is randomly chosen.
Afterwards, the corresponding subtree is entirely replaced by a new randomly
generated tree.
% Genetic operators? Pictures?

\subsection{Developmental genetic programming}

\section{Workflows}